{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Rzlg7O2Q1E7P"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NSjAiHWs1IJb",
    "outputId": "a4130bf2-e3e2-4c82-b887-c12d07fc8185"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda:0\")\n",
    "  print(\"GPU\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")\n",
    "  print(\"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lc107i9AwAc7",
    "outputId": "39182fe2-14cd-43cc-e648-4fb32f363707"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munzip -uq \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/DDD Project/Driver Drowsiness Dataset (DDD)/archive.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!unzip -uq \"/content/drive/MyDrive/DDD Project/Driver Drowsiness Dataset (DDD)/archive.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sFGa4k8w1e69",
    "outputId": "945a5725-248a-4121-f9b3-f14760cc80fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df\n",
      "(41793, 4)\n",
      "dfTest\n",
      "(3855, 4)\n",
      "dfWithout\n",
      "(37938, 4)\n",
      "testTO\n",
      "(3855, 2)\n"
     ]
    }
   ],
   "source": [
    "# Add image paths, labels, file names and person IDs into lists\n",
    "image_path, label, file_name, person_ID = [], [], [], []\n",
    "\n",
    "pattern = re.compile(r'^[a-zA-Z][a-zA-Z]?') #identification criteria of participant\n",
    "\n",
    "#data_dir = \"/content/drive/MyDrive/newdata/Driver Drowsiness Dataset (DDD)/\"\n",
    "\n",
    "path = os.getcwd()\n",
    "data_dir = path + str('/Driver Drowsiness Dataset (DDD)/')\n",
    "\n",
    "for class_name in os.listdir(data_dir):  #goes through all folders in DDD folder\n",
    "  for path in os.listdir(data_dir+class_name):  #goes through all files in both folders and updates file path and labels\n",
    "    if class_name == 'Drowsy':\n",
    "      label.append(0)\n",
    "    else:\n",
    "      label.append(1)\n",
    "    image_path.append(os.path.join(data_dir, class_name, path))\n",
    "    file_name.append(path)\n",
    "\n",
    "    person_ID.append(pattern.findall(path)[0])  #identification of participant\n",
    "\n",
    "\n",
    "#dataframe with path, label, file name, person_ID of all Persons\n",
    "df = pd.DataFrame()\n",
    "df['images'] = image_path\n",
    "df['label'] = label\n",
    "df['name'] = file_name\n",
    "df['person'] = person_ID\n",
    "\n",
    "print('df')\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "#add Person E and G only to the Test Dataset\n",
    "dfTest = pd.concat([df[df['person']=='m'], df[df['person']=='M'], df[df['person']=='u'], df[df['person']=='U'], df[df['person']=='za'], df[df['person']=='ZA']])\n",
    "print('dfTest')\n",
    "print(dfTest.shape)\n",
    "\n",
    "\n",
    "#remove Person E and G from the dataset with all Persons\n",
    "dfWithout = df\n",
    "dfWithout = dfWithout.drop(index=(df[df['person']=='m'].index))\n",
    "dfWithout = dfWithout.drop(index=(df[df['person']=='M'].index))\n",
    "dfWithout = dfWithout.drop(index=(df[df['person']=='u'].index))\n",
    "dfWithout = dfWithout.drop(index=(df[df['person']=='U'].index))\n",
    "\n",
    "dfWithout = dfWithout.drop(index=(df[df['person']=='za'].index))\n",
    "dfWithout = dfWithout.drop(index=(df[df['person']=='ZA'].index))\n",
    "\n",
    "print('dfWithout')\n",
    "print(dfWithout.shape)\n",
    "\n",
    "\n",
    "#converting imagepath and label of takeout persons G, E into an nparray\n",
    "imagePathTakeout = dfTest['images']\n",
    "labelTakeout = dfTest['label']\n",
    "\n",
    "imagePathTO = np.array(imagePathTakeout).reshape([-1,1])\n",
    "labelTO = np.array(labelTakeout).reshape([-1,1])\n",
    "testTO = np.hstack((imagePathTO, labelTO))\n",
    "\n",
    "print('testTO')\n",
    "print(testTO.shape)\n",
    "\n",
    "\n",
    "#converting imagepath and label of all other persons into an nparray\n",
    "imagePathWithout = dfWithout['images']\n",
    "labelWithout = dfWithout['label']\n",
    "\n",
    "image_path = np.array(imagePathWithout).reshape([-1,1])\n",
    "label = np.array(labelWithout).reshape([-1,1])\n",
    "comp_data = np.hstack((image_path, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bsH9As_U1fsg",
    "outputId": "a0e773ca-9c2f-4244-c7a0-3bf0bd9e991a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['/rwthfs/rz/cluster/home/ko617013/techlabs/Driver Drowsiness Dataset (DDD)/Non Drowsy/m0420.png'\n",
      "  1]\n",
      " ['/rwthfs/rz/cluster/home/ko617013/techlabs/Driver Drowsiness Dataset (DDD)/Non Drowsy/m0364.png'\n",
      "  1]\n",
      " ['/rwthfs/rz/cluster/home/ko617013/techlabs/Driver Drowsiness Dataset (DDD)/Non Drowsy/m0361.png'\n",
      "  1]\n",
      " ...\n",
      " ['/rwthfs/rz/cluster/home/ko617013/techlabs/Driver Drowsiness Dataset (DDD)/Drowsy/ZA0108.png'\n",
      "  0]\n",
      " ['/rwthfs/rz/cluster/home/ko617013/techlabs/Driver Drowsiness Dataset (DDD)/Drowsy/ZA0286.png'\n",
      "  0]\n",
      " ['/rwthfs/rz/cluster/home/ko617013/techlabs/Driver Drowsiness Dataset (DDD)/Drowsy/ZA0751.png'\n",
      "  0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(testTO)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c1VHYWqc1iBx",
    "outputId": "ca0ec5a9-f734-4d71-f1f0-8aecd887575c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "person\n",
       "A     1411\n",
       "B      315\n",
       "C      335\n",
       "D      179\n",
       "E      962\n",
       "F      415\n",
       "G      499\n",
       "H      508\n",
       "I     1095\n",
       "J      474\n",
       "K      630\n",
       "L      732\n",
       "M      777\n",
       "N     1156\n",
       "O     1097\n",
       "P      963\n",
       "Q      562\n",
       "R      204\n",
       "S      487\n",
       "T      933\n",
       "U      420\n",
       "V      653\n",
       "W     1162\n",
       "X     1749\n",
       "Y     1112\n",
       "ZA     621\n",
       "ZB    1551\n",
       "ZC    1346\n",
       "a     1252\n",
       "b      409\n",
       "c      400\n",
       "d     1005\n",
       "e     1000\n",
       "g      109\n",
       "h      571\n",
       "i     1045\n",
       "j      717\n",
       "k      538\n",
       "l      381\n",
       "m      473\n",
       "n      957\n",
       "o      671\n",
       "p      190\n",
       "q      521\n",
       "r      522\n",
       "s      457\n",
       "u      510\n",
       "v     1002\n",
       "w      493\n",
       "x     1143\n",
       "y     1500\n",
       "za    1054\n",
       "zb    1237\n",
       "zc    1288\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "df.groupby(\"person\").size()\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cCu1CDIQ1j1V",
    "outputId": "adc4c6e0-acb8-4443-dd0b-ff88fb8d9e05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainW\n",
      "(30350, 2)\n",
      "testW\n",
      "(7588, 2)\n",
      "test\n",
      "(11443, 2)\n",
      "[['/rwthfs/rz/cluster/home/ko617013/techlabs/Driver Drowsiness Dataset (DDD)/Non Drowsy/zb1246.png'\n",
      "  1]\n",
      " ['/rwthfs/rz/cluster/home/ko617013/techlabs/Driver Drowsiness Dataset (DDD)/Non Drowsy/zc1522.png'\n",
      "  1]\n",
      " ['/rwthfs/rz/cluster/home/ko617013/techlabs/Driver Drowsiness Dataset (DDD)/Drowsy/X0177.png'\n",
      "  0]\n",
      " ...\n",
      " ['/rwthfs/rz/cluster/home/ko617013/techlabs/Driver Drowsiness Dataset (DDD)/Drowsy/B0173.png'\n",
      "  0]\n",
      " ['/rwthfs/rz/cluster/home/ko617013/techlabs/Driver Drowsiness Dataset (DDD)/Drowsy/Y0727.png'\n",
      "  0]\n",
      " ['/rwthfs/rz/cluster/home/ko617013/techlabs/Driver Drowsiness Dataset (DDD)/Drowsy/J0332.png'\n",
      "  0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#nparray with all other persons (except of E, G) gets split into train and test dataset\n",
    "trainW, testW = train_test_split(comp_data, test_size=0.2, random_state=1234)\n",
    "print('trainW')\n",
    "print(trainW.shape)\n",
    "\n",
    "print('testW')\n",
    "print(testW.shape)\n",
    "\n",
    "\n",
    "#train dataset\n",
    "train = trainW\n",
    "\n",
    "\n",
    "#test dataset is a combination of the takeout of person E, G and the part that was split to test via train_test_split\n",
    "test = np.concatenate((testTO, testW))\n",
    "\n",
    "\n",
    "print('test')\n",
    "print(test.shape)\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "H6WoJ6ET1mFK"
   },
   "outputs": [],
   "source": [
    "class DDDdataset(Dataset):\n",
    "  def __init__(self, image_paths, image_labels, transform):\n",
    "    super().__init__()\n",
    "    self.paths = image_paths\n",
    "    self.labels = image_labels\n",
    "    self.len = len(self.paths)\n",
    "    self.transform = transform\n",
    "  \n",
    "  def __len__(self): return self.len\n",
    "  \n",
    "  def __getitem__(self, index):\n",
    "    path = self.paths[index]\n",
    "    label = self.labels[index]\n",
    "    target_tensor=torch.from_numpy(np.array(label,dtype=np.int16))\n",
    "    #image = Image.open(path).convert('RGB')\n",
    "    image = cv2.imread(path,cv2.IMREAD_UNCHANGED)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = self.transform(image)\n",
    "    return (image, target_tensor, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "enP-t4n-1oXH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#transformation for the calculation of mean and std for the train dataset\\ntransformationCalc = transforms.Compose([\\n    #ToTensor() includes an automatic scaling from the interval (0,255) to (0.0,1.0)\\n    transforms.ToTensor()\\n])\\n\\n#train dataset and loader for the calculation of mean and std for the train dataset\\ntrainDatasetCalc = DDDdataset(train[:,0], train[:,-1], transformationCalc)\\n\\nloaderCalc = DataLoader(trainDatasetCalc, batch_size=128, shuffle=True)'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"#transformation for the calculation of mean and std for the train dataset\n",
    "transformationCalc = transforms.Compose([\n",
    "    #ToTensor() includes an automatic scaling from the interval (0,255) to (0.0,1.0)\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "#train dataset and loader for the calculation of mean and std for the train dataset\n",
    "trainDatasetCalc = DDDdataset(train[:,0], train[:,-1], transformationCalc)\n",
    "\n",
    "loaderCalc = DataLoader(trainDatasetCalc, batch_size=128, shuffle=True)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "id": "nsHPACDAqezb",
    "outputId": "09fd2119-795e-4631-f3b2-1cf3fb5e136e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sum = []\\nsum = np.zeros((3))\\nno_of_pixels = len(trainDatasetCalc)*227*227\\nfor batch in loaderCalc: \\n  \\n\\n  for i in range(3) : #no of channels\\n    \\n  \\n    sum[i] += batch[0][:,i,:,:].sum()\\n  '"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"sum = []\n",
    "sum = np.zeros((3))\n",
    "no_of_pixels = len(trainDatasetCalc)*227*227\n",
    "for batch in loaderCalc: \n",
    "  \n",
    "\n",
    "  for i in range(3) : #no of channels\n",
    "    \n",
    "  \n",
    "    sum[i] += batch[0][:,i,:,:].sum()\n",
    "  \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "e2_c6oITryiF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mean = sum / no_of_pixels\\nprint(mean)'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"mean = sum / no_of_pixels\n",
    "print(mean)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "gAauqWPLwGWl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sum_sq = []\\nsum_sq = np.zeros((3))\\nfor batch in loaderCalc:\\n  for i in range(3) : #no of channels\\n      \\n      sum_sq[i] += ((batch[0][:,i,:,:]-mean[i]).pow(2)).sum()'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"sum_sq = []\n",
    "sum_sq = np.zeros((3))\n",
    "for batch in loaderCalc:\n",
    "  for i in range(3) : #no of channels\n",
    "      \n",
    "      sum_sq[i] += ((batch[0][:,i,:,:]-mean[i]).pow(2)).sum()\"\"\"\n",
    "      \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "b8F5rJDyw6TO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'std = np.sqrt(sum_sq/no_of_pixels)\\nprint(std)'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"std = np.sqrt(sum_sq/no_of_pixels)\n",
    "print(std)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "DVmw9m11GB05"
   },
   "outputs": [],
   "source": [
    "mean = [0.5055243 , 0.3837401 , 0.33682286]\n",
    "std = [0.24825575, 0.22582591 ,0.21188137]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "6mj8MX1Y1v4u"
   },
   "outputs": [],
   "source": [
    "#transformation for model training and testing\n",
    "transformation = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "\n",
    "#datasets and dataloaders for model training and testing\n",
    "trainDataset = DDDdataset(train[:,0], train[:,-1], transformation)\n",
    "testDataset = DDDdataset(test[:,0], test[:,-1], transformation)\n",
    "\n",
    "trainDataLoader = DataLoader(trainDataset, batch_size=64, shuffle=True, num_workers = 2)\n",
    "testDataLoader = DataLoader(testDataset, batch_size=64, shuffle=True, num_workers = 2)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "ZBgfaAOv1yA4",
    "outputId": "8ac1ca60-9959-4714-974c-9701aca4ad38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n--------------------------------\\nnumber of parameters CNN below\\n\\n(II) f*f*ch_in*ch_out + ch_out_bias = params_conv_layer\\n\\n(III) numberOfInputImageElements*outputFeaturesCurrentFCLayer = params_fully_connected\\n\\nconv1: 4*4*3*10 + 10 = 490 -> 4*4 filter, 3 input image channels, 10 number of filters, +10 bias parameters of each filter\\nconv2: 5*5*10*20 + 20 = 5020\\nconv3: 6*6*20*32 + 32 = 23072\\nconv4: 3*3*32*40 + 40 = 11560\\n\\nafter conv4 layer: output \"image\": 3*3*40 = 360\\n\\nfc1: 360*50 + 50 = 18050 -> fc1 has 50 output features\\nfc2: 50*20 + 20 = 1020\\nfc3: 20*2 + 2 = 42\\n\\ncomplete CNN: 59254 parameters\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "--------------------------------\n",
    "number of parameters CNN below\n",
    "\n",
    "(II) f*f*ch_in*ch_out + ch_out_bias = params_conv_layer\n",
    "\n",
    "(III) numberOfInputImageElements*outputFeaturesCurrentFCLayer = params_fully_connected\n",
    "\n",
    "conv1: 4*4*3*10 + 10 = 490 -> 4*4 filter, 3 input image channels, 10 number of filters, +10 bias parameters of each filter\n",
    "conv2: 5*5*10*20 + 20 = 5020\n",
    "conv3: 6*6*20*32 + 32 = 23072\n",
    "conv4: 3*3*32*40 + 40 = 11560\n",
    "\n",
    "after conv4 layer: output \"image\": 3*3*40 = 360\n",
    "\n",
    "fc1: 360*50 + 50 = 18050 -> fc1 has 50 output features\n",
    "fc2: 50*20 + 20 = 1020\n",
    "fc3: 20*2 + 2 = 42\n",
    "\n",
    "complete CNN: 59254 parameters\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "saaBxfbB10UL"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#LAYERS OF THE MODEL\n",
    "\n",
    "class CNN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=10, kernel_size=(4,4), stride=1, padding=1)\n",
    "    self.bn1 = nn.BatchNorm2d(10)\n",
    "    self.conv2 = nn.Conv2d(in_channels=10, out_channels=15, kernel_size=(5,5), stride=2, padding=1)\n",
    "    self.bn2 = nn.BatchNorm2d(15)\n",
    "    self.conv3 = nn.Conv2d(in_channels=15, out_channels=20, kernel_size=(6,6), stride=2, padding=1)\n",
    "    self.bn3 = nn.BatchNorm2d(20)\n",
    "    self.conv4 = nn.Conv2d(in_channels=20, out_channels=25, kernel_size=(3,3), stride=1, padding=1)\n",
    "    self.bn4 = nn.BatchNorm2d(25)\n",
    "\n",
    "    self.fc1 = nn.Linear(in_features=25*3*3, out_features=45)\n",
    "    self.bn5 = nn.BatchNorm1d(45)\n",
    "    self.fc2 = nn.Linear(in_features=45, out_features=20)\n",
    "    self.bn6 = nn.BatchNorm1d(20)\n",
    "    self.fc3 = nn.Linear(in_features=20, out_features=2)\n",
    "\n",
    "  def forward(self, X):\n",
    "\n",
    "    X = F.relu(self.bn1(self.conv1(X)))\n",
    "    X = F.max_pool2d(X, 2)\n",
    "    X = F.relu(self.bn2(self.conv2(X)))\n",
    "    X = F.max_pool2d(X, 2)\n",
    "    X = F.relu(self.bn3(self.conv3(X)))\n",
    "    X = F.max_pool2d(X, 3, stride=2)\n",
    "    X = F.relu(self.bn4(self.conv4(X)))\n",
    "    X = F.max_pool2d(X, 2)\n",
    "\n",
    "    X = X.view(X.shape[0], -1)\n",
    "    X = F.relu(self.bn5(self.fc1(X)))\n",
    "    X = F.relu(self.bn6(self.fc2(X)))\n",
    "    X = self.fc3(X)\n",
    "\n",
    "    return X\n",
    "\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wcTlnEDU12ly",
    "outputId": "ed6d8e73-16b6-4db3-889d-d90dd6dd4aa6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " --- Epoch: 0, train loss: 0.0531, train acc: 0.9920, f1 score: 0.9920,  time: 99.99681353569031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/w0/tmp/slurm_ko617013.34284693/ipykernel_115711/510457353.py:111: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if rowsOfWrongClassifications == []:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, test loss: 0.3171, test acc: 0.8594, f1score: 0.8609, time: 216.20326328277588\n",
      "\n",
      "\n",
      " --- Epoch: 1, train loss: 0.0057, train acc: 0.9989, f1 score: 0.9989,  time: 317.73957443237305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/w0/tmp/slurm_ko617013.34284693/ipykernel_115711/510457353.py:111: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if rowsOfWrongClassifications == []:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, test loss: 0.3839, test acc: 0.8685, f1score: 0.8685, time: 355.0184235572815\n",
      "\n",
      "\n",
      " --- Epoch: 2, train loss: 0.0054, train acc: 0.9983, f1 score: 0.9983,  time: 456.00800013542175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/w0/tmp/slurm_ko617013.34284693/ipykernel_115711/510457353.py:111: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if rowsOfWrongClassifications == []:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, test loss: 0.5016, test acc: 0.8194, f1score: 0.8202, time: 494.7646007537842\n",
      "\n",
      "\n",
      " --- Epoch: 3, train loss: 0.0024, train acc: 0.9995, f1 score: 0.9995,  time: 597.5126535892487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/w0/tmp/slurm_ko617013.34284693/ipykernel_115711/510457353.py:111: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if rowsOfWrongClassifications == []:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, test loss: 0.5138, test acc: 0.8593, f1score: 0.8593, time: 634.3870198726654\n",
      "\n",
      "\n",
      " --- Epoch: 4, train loss: 0.0008, train acc: 0.9998, f1 score: 0.9998,  time: 736.7147097587585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/w0/tmp/slurm_ko617013.34284693/ipykernel_115711/510457353.py:111: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if rowsOfWrongClassifications == []:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, test loss: 0.3088, test acc: 0.8830, f1score: 0.8830, time: 775.0820338726044\n",
      "\n",
      "\n",
      " --- Epoch: 5, train loss: 0.0004, train acc: 1.0000, f1 score: 1.0000,  time: 875.783988237381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/w0/tmp/slurm_ko617013.34284693/ipykernel_115711/510457353.py:111: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if rowsOfWrongClassifications == []:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, test loss: 0.3655, test acc: 0.8843, f1score: 0.8847, time: 913.2072224617004\n",
      "\n",
      "\n",
      " --- Epoch: 6, train loss: 0.0074, train acc: 0.9981, f1 score: 0.9981,  time: 1013.7582771778107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/w0/tmp/slurm_ko617013.34284693/ipykernel_115711/510457353.py:111: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if rowsOfWrongClassifications == []:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, test loss: 0.2029, test acc: 0.9270, f1score: 0.9270, time: 1050.5757746696472\n",
      "\n",
      "\n",
      " --- Epoch: 7, train loss: 0.0009, train acc: 0.9999, f1 score: 0.9999,  time: 1152.21777176857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/w0/tmp/slurm_ko617013.34284693/ipykernel_115711/510457353.py:111: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if rowsOfWrongClassifications == []:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, test loss: 0.2947, test acc: 0.9346, f1score: 0.9346, time: 1190.0217487812042\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#MODEL TRAINING AND TESTING\n",
    "\n",
    "import time\n",
    "\n",
    "#basic settings\n",
    "model = CNN().to(device)\n",
    "number_epochs = 8\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "train_preds = []\n",
    "test_preds_list = []\n",
    "\n",
    "train_class = []\n",
    "test_class = []\n",
    "\n",
    "#for the analysis of the wrong classifications\n",
    "pathsOfWrongClassifications = []\n",
    "rowsOfWrongClassifications = []\n",
    "correctLabel = []\n",
    "epochOfWrongClassification = []\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "#model training and testing process\n",
    "for epoch in range(number_epochs):\n",
    "  train_preds = []\n",
    "  test_preds_list = []\n",
    "\n",
    "  train_class = []\n",
    "  test_class = []\n",
    "\n",
    "  epoch_loss = 0\n",
    "  epoch_accuracy = 0\n",
    "  \n",
    "  #-------model training-------\n",
    "  for (X, y, path) in trainDataLoader:\n",
    "    X = X.to(device) #torch.as_tensor(X)\n",
    "    y = y.to(device) #torch.as_tensor(y)\n",
    "    train_class.extend(y.cpu().tolist())   #list of true labels\n",
    "    \n",
    "    predictions = model(X)\n",
    "    \n",
    "    \n",
    "    pred_class = predictions.argmax(dim=1)\n",
    "    train_preds.extend(pred_class.cpu().tolist())    #list of predicted labels\n",
    "    \n",
    "    \n",
    "    \n",
    "    loss = loss_func(predictions, y.long())\n",
    "\n",
    "    #adaption of the neural network\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    #\n",
    "    accuracy = ((predictions.argmax(dim=1) == y).float().mean())\n",
    "    \n",
    "    epoch_accuracy += accuracy\n",
    "    epoch_loss += loss\n",
    "  \n",
    "  #total epoch_accuracy and epoch_loss calculation\n",
    "  epoch_accuracy = epoch_accuracy/len(trainDataLoader)\n",
    "  accuracies.append(epoch_accuracy)\n",
    "  epoch_loss = epoch_loss / len(trainDataLoader)\n",
    "  losses.append(epoch_loss)\n",
    "  #print(\"predictions \", train_preds)\n",
    "  #print(\"class \", train_class)\n",
    "\n",
    "  f1score = f1_score(train_preds, train_class, average = 'weighted')\n",
    "  #print(\"f1score \", f1score)\n",
    "  print(\"\\n --- Epoch: {}, train loss: {:.4f}, train acc: {:.4f}, f1 score: {:.4f},  time: {}\".format(epoch, epoch_loss, epoch_accuracy, f1score, time.time() - start))\n",
    "  #-------model training-------\n",
    "  \n",
    "  \n",
    "  #---model testing---\n",
    "  model.eval() #sets the model in evaluation mode\n",
    "  \n",
    "  #with disabled gradient calculation\n",
    "  with torch.no_grad():\n",
    "    test_epoch_loss = 0\n",
    "    test_epoch_accuracy = 0\n",
    "\n",
    "    for test_X, test_y, path in testDataLoader:\n",
    "\n",
    "      test_X = test_X.to(device)\n",
    "      test_y = test_y.to(device)\n",
    "      test_class.extend(test_y.cpu().tolist())   #list of true labels\n",
    "\n",
    "      test_preds = model(test_X)\n",
    "      \n",
    "      test_pred_class = test_preds.argmax(dim=1)  \n",
    "      test_preds_list.extend(test_pred_class.cpu().tolist())    #list of predicted labels\n",
    "    \n",
    "    \n",
    "      test_loss = loss_func(test_preds, test_y.long())\n",
    "      \n",
    "      \n",
    "        \n",
    "      test_epoch_loss += test_loss            \n",
    "      test_accuracy = ((test_preds.argmax(dim=1) == test_y).float().mean())\n",
    "      test_epoch_accuracy += test_accuracy\n",
    "\n",
    "      #if test prediction is wrong save the path of the image that has been classified wrong\n",
    "      for index, row in enumerate(test_preds):\n",
    "        if row.argmax(dim=0) != test_y[index]:\n",
    "          pathsOfWrongClassifications.append(path[index])\n",
    "          if rowsOfWrongClassifications == []:\n",
    "            row = row.cpu()\n",
    "            rowsOfWrongClassifications = row\n",
    "          else:\n",
    "            row = row.cpu()\n",
    "            rowsOfWrongClassifications = np.vstack([rowsOfWrongClassifications, row])\n",
    "          correctLabel.append(test_y[index].item())\n",
    "          epochOfWrongClassification.append(epoch)\n",
    "\n",
    "    #total test_epoch_accuracy and test_epoch_loss calculation\n",
    "    test_epoch_accuracy = test_epoch_accuracy/len(testDataLoader)\n",
    "    test_epoch_loss = test_epoch_loss / len(testDataLoader)\n",
    "    #print(\"test predictions \", test_preds_list)\n",
    "    #print(\"test predictions class \", test_class)\n",
    "    f1score_test = f1_score(test_preds_list, test_class, average = 'weighted')\n",
    "    \n",
    "    print(\"Epoch: {}, test loss: {:.4f}, test acc: {:.4f}, f1score: {:.4f}, time: {}\\n\".format(epoch, test_epoch_loss, test_epoch_accuracy,f1score_test, time.time() - start))\n",
    "  #---model testing---\n",
    "\n",
    "  model.train(mode=True) #sets the model back in training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "LxH3HzsW15YR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       testEpochNo personID  \\\n",
      "0                0        U   \n",
      "1                0       za   \n",
      "2                0        u   \n",
      "3                0       za   \n",
      "4                0        u   \n",
      "...            ...      ...   \n",
      "11033            7        u   \n",
      "11034            7        u   \n",
      "11035            7        U   \n",
      "11036            7        U   \n",
      "11037            7        U   \n",
      "\n",
      "                                               imagepath  correctLabel  \\\n",
      "0      /rwthfs/rz/cluster/home/ko617013/techlabs/Driv...             0   \n",
      "1      /rwthfs/rz/cluster/home/ko617013/techlabs/Driv...             1   \n",
      "2      /rwthfs/rz/cluster/home/ko617013/techlabs/Driv...             1   \n",
      "3      /rwthfs/rz/cluster/home/ko617013/techlabs/Driv...             1   \n",
      "4      /rwthfs/rz/cluster/home/ko617013/techlabs/Driv...             1   \n",
      "...                                                  ...           ...   \n",
      "11033  /rwthfs/rz/cluster/home/ko617013/techlabs/Driv...             1   \n",
      "11034  /rwthfs/rz/cluster/home/ko617013/techlabs/Driv...             1   \n",
      "11035  /rwthfs/rz/cluster/home/ko617013/techlabs/Driv...             0   \n",
      "11036  /rwthfs/rz/cluster/home/ko617013/techlabs/Driv...             0   \n",
      "11037  /rwthfs/rz/cluster/home/ko617013/techlabs/Driv...             0   \n",
      "\n",
      "       outputValueLabel0  outputValueLabel1  \n",
      "0              -1.181964           1.065189  \n",
      "1               1.646669          -1.301059  \n",
      "2               0.680430          -0.911473  \n",
      "3               0.754205          -0.560010  \n",
      "4               0.726451          -0.871330  \n",
      "...                  ...                ...  \n",
      "11033           0.077842           0.018758  \n",
      "11034           1.609139          -1.381555  \n",
      "11035          -2.096491           1.684857  \n",
      "11036          -4.882831           4.208928  \n",
      "11037          -2.813960           2.354989  \n",
      "\n",
      "[11038 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "#np_rows = np.array(rowsOfWrongClassifications)\n",
    "\n",
    "#extract the person ID out of the stored image paths\n",
    "personID = []\n",
    "for pathStr in pathsOfWrongClassifications:\n",
    "  personID.append(pathStr[pathStr.rfind('/')+1:-8])\n",
    "\n",
    "\n",
    "#create a dataframe from all wrong classified images in the test dataset\n",
    "dfWrong = pd.DataFrame()\n",
    "dfWrong['testEpochNo'] = epochOfWrongClassification\n",
    "dfWrong['personID'] = personID\n",
    "dfWrong['imagepath'] = pathsOfWrongClassifications\n",
    "dfWrong['correctLabel'] = correctLabel\n",
    "dfWrong['outputValueLabel0'] = rowsOfWrongClassifications[:,0]\n",
    "dfWrong['outputValueLabel1'] = rowsOfWrongClassifications[:,1]\n",
    "\n",
    "print(dfWrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "olPaZiFA19-U"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "personID\n",
       "C       7\n",
       "D       1\n",
       "E       1\n",
       "K       1\n",
       "M     751\n",
       "U     219\n",
       "ZA    562\n",
       "m      66\n",
       "s       1\n",
       "u     454\n",
       "za      4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get an overview of the persons that are likely to be classified in a wrong way\n",
    "dfOfEpoch2 = dfWrong[dfWrong['testEpochNo']==2]\n",
    "dfOfEpoch2.groupby('personID').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
