{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mattac22/TechLabs_DriverDrowsinessDetection/blob/maroof/DDD_Basic_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#INFORMATION ABOUT CNN\n",
        "\n",
        "#TRANSFORMATION scales from interval [0, 255] to [0.0, 1.0] and then Normalization x_new = (x-mean)/std\n",
        "\n",
        "#BATCH_SIZE = 64\n",
        "\n",
        "#----NUMBER OF PARAMETERS CNN----\n",
        "\n",
        "# input image has size 227*227\n",
        "\n",
        "#conv1: 4*4*3*10 + 10 = 490 -> 4*4 filter, 3 input image channels, 10 number of filters, +10 bias parameters of each filter\n",
        "#conv2: 5*5*10*15 + 15 = 3765\n",
        "#conv3: 6*6*15*20 + 20 = 10820\n",
        "#conv4: 3*3*20*25 + 25 = 4525\n",
        "\n",
        "#after conv4 layer: output \"image\": 3*3*25 = 225\n",
        "\n",
        "#fc1: 225*45 + 45 = 10170 -> fc1 has 45 output features\n",
        "#fc2: 45*20 + 20 = 920\n",
        "#fc3: 20*2 + 2 = 42\n",
        "\n",
        "#TOTAL CNN: 30732 parameters\n",
        "\n",
        "#----NUMBER OF PARAMETERS CNN----\n",
        "\n",
        "#---- CURRENT RESULTS ----\n",
        "\n",
        "# --- Epoch: 0, train loss: 0.0547, train acc: 0.9920, time: 185.72551941871643\n",
        "#Epoch: 0, test loss: 0.5021, test acc: 0.8298, time: 982.3053231239319\n",
        "\n",
        "# --- Epoch: 1, train loss: 0.0054, train acc: 0.9990, time: 1167.7591831684113\n",
        "#Epoch: 1, test loss: 0.3759, test acc: 0.9193, time: 1233.2979555130005\n",
        "\n",
        "# --- Epoch: 2, train loss: 0.0047, train acc: 0.9988, time: 1410.846270084381\n",
        "#Epoch: 2, test loss: 1.0650, test acc: 0.8194, time: 1476.1876842975616\n",
        "\n",
        "# --- Epoch: 3, train loss: 0.0022, train acc: 0.9993, time: 1649.150539636612\n",
        "#Epoch: 3, test loss: 0.6330, test acc: 0.8666, time: 1713.3415627479553\n",
        "\n",
        "# --- Epoch: 4, train loss: 0.0003, train acc: 1.0000, time: 1887.5666482448578\n",
        "#Epoch: 4, test loss: 0.8888, test acc: 0.8573, time: 1951.4360206127167\n",
        "\n",
        "# --- Epoch: 5, train loss: 0.0040, train acc: 0.9987, time: 2124.932951927185\n",
        "#Epoch: 5, test loss: 0.6418, test acc: 0.8751, time: 2189.122528076172\n",
        "\n",
        "# --- Epoch: 6, train loss: 0.0012, train acc: 0.9996, time: 2360.4124615192413\n",
        "#Epoch: 6, test loss: 0.7349, test acc: 0.8451, time: 2422.3692615032196\n",
        "\n",
        "# --- Epoch: 7, train loss: 0.0002, train acc: 1.0000, time: 2594.427927017212\n",
        "#Epoch: 7, test loss: 0.5947, test acc: 0.9201, time: 2659.300325155258\n",
        "\n",
        "#---- ----CLASSIFIED WRONG---- ----\n",
        "#person E, G, S have been shifted only to the test dataset\n",
        "\n",
        "#persons in test dataset that have been classified in a wrong way\n",
        "\n",
        "#E    278/962  = 28.9%\n",
        "#e     34/1000 = 3.4%\n",
        "\n",
        "#G      5/499  = 1.0%\n",
        "#g    108/109  = 99.1%\n",
        "\n",
        "#S      8/487  = 1.6%\n",
        "#s    457/457  = 100.0%\n",
        "\n",
        "#k      1/538  = 0.19%\n",
        "\n",
        "#---- CURRENT RESULTS ----"
      ],
      "metadata": {
        "id": "JmpTgUSGkm7F"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BFiWhGo12e_o"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#set the device\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda:0\")\n",
        "  print(\"GPU\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "  print(\"CPU\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cpG-AJu1Pq9",
        "outputId": "a656cb10-1652-467b-bae2-49ed41858383"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14NnMBFNzg8S",
        "outputId": "0f99bd39-9eb6-4bf2-d0f8-13a65562669c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.10.7)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (16.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.15.1+cu118)\n",
            "Requirement already satisfied: torch==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (2.0.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0->torchvision) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0->torchvision) (1.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0->torchvision) (3.10.7)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0->torchvision) (3.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0->torchvision) (2.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0->torchvision) (3.1.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch==2.0.0->torchvision) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch==2.0.0->torchvision) (16.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch==2.0.0->torchvision) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch==2.0.0->torchvision) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8UYw-3Q_oVe",
        "outputId": "bf921c43-e603-4f6b-c6c7-32d92ae50572"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "#mount google drive for having access to the dataset in google colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-oL7Nj8bmmAj"
      },
      "outputs": [],
      "source": [
        "#unzip if files have not been unzipped before\n",
        "!unzip -uq \"/content/drive/MyDrive/Driver_Drowsiness/Driver_Drowsiness_Dataset.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b0f7915-ffc5-4af0-ef00-4e1783ad94b2",
        "id": "MB0WOdU9xQeY"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df\n",
            "(41793, 4)\n",
            "dfTest\n",
            "(3514, 4)\n",
            "dfWithout\n",
            "(38039, 4)\n",
            "testTO\n",
            "(3514, 2)\n"
          ]
        }
      ],
      "source": [
        "#DATAFRAME OF ALL PICTURES, TAKEOUT SPECIFIC PERSONS, \n",
        "\n",
        "#create a pandas dataframe out of all pictures in the dataset\n",
        "#---- ---- ----\n",
        "# Add image paths, labels, file names and person IDs into lists\n",
        "image_path, label, file_name, person_ID = [], [], [], []\n",
        "\n",
        "#for the person_ID - identification criteria of participant\n",
        "pattern = re.compile(r'^[a-zA-Z][a-zA-Z]?')\n",
        "\n",
        "data_dir = \"Driver Drowsiness Dataset (DDD)\"\n",
        "\n",
        "for class_name in os.listdir(\"Driver Drowsiness Dataset (DDD)\"):  #goes through all folders in DDD folder\n",
        "  for path in os.listdir(\"Driver Drowsiness Dataset (DDD)/\"+class_name):  #goes through all files in both folders and updates file path and labels\n",
        "    if class_name == 'Drowsy':\n",
        "      label.append(0)\n",
        "    else:\n",
        "      label.append(1)\n",
        "    image_path.append(os.path.join(data_dir, class_name, path))\n",
        "    file_name.append(path)\n",
        "\n",
        "    person_ID.append(pattern.findall(path)[0])  #identification of participant\n",
        "\n",
        "\n",
        "#dataframe with path, label, file name, person_ID of all Persons\n",
        "df = pd.DataFrame()\n",
        "df['images'] = image_path\n",
        "df['label'] = label\n",
        "df['name'] = file_name\n",
        "df['person'] = person_ID\n",
        "\n",
        "print('df')\n",
        "print(df.shape)\n",
        "#---- ---- ----\n",
        "\n",
        "#split pandas dataframe with all pictures into dfTest with person E, G, S and dfWithout with all pictures except of E, G, S\n",
        "#---- ---- ---- ---- ---- ----\n",
        "#add Person E and G only to the Test Dataset\n",
        "dfTest = pd.concat([df[df['person']=='e'], df[df['person']=='g'], df[df['person']=='E'], df[df['person']=='G'], df[df['person']=='S'], df[df['person']=='s']])\n",
        "print('dfTest')\n",
        "print(dfTest.shape)\n",
        "\n",
        "\n",
        "#remove Person E and G from the dataset with all Persons\n",
        "dfWithout = df\n",
        "dfWithout = dfWithout.drop(index=(df[df['person']=='e'].index))\n",
        "dfWithout = dfWithout.drop(index=(df[df['person']=='E'].index))\n",
        "dfWithout = dfWithout.drop(index=(df[df['person']=='g'].index))\n",
        "dfWithout = dfWithout.drop(index=(df[df['person']=='G'].index))\n",
        "\n",
        "dfWithout = dfWithout.drop(index=(df[df['person']=='d'].index))\n",
        "dfWithout = dfWithout.drop(index=(df[df['person']=='D'].index))\n",
        "\n",
        "print('dfWithout')\n",
        "print(dfWithout.shape)\n",
        "#---- ---- ---- ---- ---- ----\n",
        "\n",
        "\n",
        "#converting imagepath and label of takeout persons G, E, S into an nparray\n",
        "#----\n",
        "imagePathTakeout = dfTest['images']\n",
        "labelTakeout = dfTest['label']\n",
        "\n",
        "imagePathTO = np.array(imagePathTakeout).reshape([-1,1])\n",
        "labelTO = np.array(labelTakeout).reshape([-1,1])\n",
        "testTO = np.hstack((imagePathTO, labelTO))\n",
        "\n",
        "print('testTO')\n",
        "print(testTO.shape)\n",
        "#----\n",
        "\n",
        "\n",
        "#converting imagepath and label of all other persons into an nparray\n",
        "#---- ----\n",
        "imagePathWithout = dfWithout['images']\n",
        "labelWithout = dfWithout['label']\n",
        "\n",
        "image_path = np.array(imagePathWithout).reshape([-1,1])\n",
        "label = np.array(labelWithout).reshape([-1,1])\n",
        "comp_data = np.hstack((image_path, label))\n",
        "#---- ----"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(testTO)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6CcT4Sknfqe",
        "outputId": "c1df9008-b9b8-4888-9c04-eb4425ca4474"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Driver Drowsiness Dataset (DDD)/Non Drowsy/e0860.png' 1]\n",
            " ['Driver Drowsiness Dataset (DDD)/Non Drowsy/e0210.png' 1]\n",
            " ['Driver Drowsiness Dataset (DDD)/Non Drowsy/e0303.png' 1]\n",
            " ...\n",
            " ['Driver Drowsiness Dataset (DDD)/Non Drowsy/s0185.png' 1]\n",
            " ['Driver Drowsiness Dataset (DDD)/Non Drowsy/s0100.png' 1]\n",
            " ['Driver Drowsiness Dataset (DDD)/Non Drowsy/s0257.png' 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#number of images of ALL persons of the dataset\n",
        "df.groupby(\"person\").size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xzp6U8rpMrpW",
        "outputId": "7aeb787b-a868-4c23-e94c-e798fc68f434"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "person\n",
              "A     1411\n",
              "B      315\n",
              "C      335\n",
              "D      179\n",
              "E      962\n",
              "F      415\n",
              "G      499\n",
              "H      508\n",
              "I     1095\n",
              "J      474\n",
              "K      630\n",
              "L      732\n",
              "M      777\n",
              "N     1156\n",
              "O     1097\n",
              "P      963\n",
              "Q      562\n",
              "R      204\n",
              "S      487\n",
              "T      933\n",
              "U      420\n",
              "V      653\n",
              "W     1162\n",
              "X     1749\n",
              "Y     1112\n",
              "ZA     621\n",
              "ZB    1551\n",
              "ZC    1346\n",
              "a     1252\n",
              "b      409\n",
              "c      400\n",
              "d     1005\n",
              "e     1000\n",
              "g      109\n",
              "h      571\n",
              "i     1045\n",
              "j      717\n",
              "k      538\n",
              "l      381\n",
              "m      473\n",
              "n      957\n",
              "o      671\n",
              "p      190\n",
              "q      521\n",
              "r      522\n",
              "s      457\n",
              "u      510\n",
              "v     1002\n",
              "w      493\n",
              "x     1143\n",
              "y     1500\n",
              "za    1054\n",
              "zb    1237\n",
              "zc    1288\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7m9YAiGrun_S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55803e96-b773-4095-a642-822e78d19482"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainW\n",
            "(30431, 2)\n",
            "testW\n",
            "(7608, 2)\n",
            "test\n",
            "(11122, 2)\n",
            "[['Driver Drowsiness Dataset (DDD)/Non Drowsy/i0470.png' 1]\n",
            " ['Driver Drowsiness Dataset (DDD)/Non Drowsy/b0018.png' 1]\n",
            " ['Driver Drowsiness Dataset (DDD)/Non Drowsy/n0921.png' 1]\n",
            " ...\n",
            " ['Driver Drowsiness Dataset (DDD)/Drowsy/N1081.png' 0]\n",
            " ['Driver Drowsiness Dataset (DDD)/Drowsy/I0113.png' 0]\n",
            " ['Driver Drowsiness Dataset (DDD)/Drowsy/Y0264.png' 0]]\n"
          ]
        }
      ],
      "source": [
        "#CREATE TRAIN DATASET AND TEST DATASET\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "#nparray with all other persons (except of E, G, S) gets split into train and test dataset\n",
        "trainW, testW = train_test_split(comp_data, test_size=0.2, random_state=1234)\n",
        "print('trainW')\n",
        "print(trainW.shape)\n",
        "\n",
        "print('testW')\n",
        "print(testW.shape)\n",
        "\n",
        "\n",
        "#train dataset\n",
        "train = trainW\n",
        "\n",
        "\n",
        "#test dataset is a combination of the takeout of person E, G, S and the part that was split to test via train_test_split\n",
        "test = np.concatenate((testTO, testW))\n",
        "\n",
        "\n",
        "print('test')\n",
        "print(test.shape)\n",
        "print(train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#basic methods of Dataset class need to be overwritten\n",
        "class DDDdataset(Dataset):\n",
        "  def __init__(self, image_paths, image_labels, transform):\n",
        "    super().__init__()\n",
        "    self.paths = image_paths\n",
        "    self.labels = image_labels\n",
        "    self.len = len(self.paths)\n",
        "    self.transform = transform\n",
        "  \n",
        "  def __len__(self): return self.len\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    path = self.paths[index]\n",
        "    label = self.labels[index]\n",
        "    target_tensor=torch.from_numpy(np.array(label,dtype=np.int16))\n",
        "    #image = Image.open(path).convert('RGB')\n",
        "    image = cv2.imread(path,cv2.IMREAD_UNCHANGED)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image = self.transform(image)\n",
        "    return (image, target_tensor, path)"
      ],
      "metadata": {
        "id": "CtupXG87cCc4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TRANSFORMATION, TRAIN DATASET AND LOADER FOR THE CALCULATION OF MEAN AND STD FOR EVERY CHANNEL\n",
        "\n",
        "#transformation for the calculation of mean and std for the train dataset\n",
        "transformationCalc = transforms.Compose([\n",
        "    #ToTensor() includes an automatic scaling from the interval (0,255) to (0.0,1.0)\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "#train dataset and loader for the calculation of mean and std for the train dataset\n",
        "trainDatasetCalc = DDDdataset(train[:,0], train[:,-1], transformationCalc)\n",
        "\n",
        "loaderCalc = DataLoader(trainDatasetCalc, batch_size=128, shuffle=True)"
      ],
      "metadata": {
        "id": "ddyjWR-3cJzl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CALCULATION MEAN AND STD FOR EVERY CHANNEL\n",
        "\n",
        "numberOfPixelsPerImage = 227*227\n",
        "noOfImages = train.shape[0]\n",
        "\n",
        "\n",
        "#lists to collect the sum of all pixel values in the batch for R, G, B\n",
        "sumOfBatchesR, sumOfBatchesG, sumOfBatchesB = [], [], []\n",
        "squaredSumOfBatchesR, squaredSumOfBatchesG, squaredSumOfBatchesB = [], [], []\n",
        "\n",
        "\n",
        "#in calcBatch(es) with 128 pictures each perform sum operations\n",
        "for calcBatch in loaderCalc:\n",
        "  #calcBatch[0] with dimension (128, 3, 227, 227)\n",
        "  currentBatch = calcBatch[0]\n",
        "  #currentBatch = currentBatch.float()\n",
        "\n",
        "\n",
        "  rgbSumOfBatch = currentBatch.sum(axis=3) #reduce column of one picture to one column (128, 3, 227)\n",
        "  rgbSumOfBatch = rgbSumOfBatch.sum(axis=2) #reduce row of one picture to one row (128, 3)\n",
        "  rgbSumOfBatch = rgbSumOfBatch.sum(axis=0) #combine all pictures of the batch to 3 values for rgb (3)\n",
        "\n",
        "  #sum of the batch for R, G, B is stored for final calculation\n",
        "  sumOfBatchesR.append(rgbSumOfBatch[0])\n",
        "  sumOfBatchesG.append(rgbSumOfBatch[1])\n",
        "  sumOfBatchesB.append(rgbSumOfBatch[2])\n",
        "\n",
        "\n",
        "  rgbSquaredSumOfBatch = ((currentBatch)**2).sum(axis=3) #reduce column of one picture to one column (128, 227, 3)\n",
        "  rgbSquaredSumOfBatch = rgbSquaredSumOfBatch.sum(axis=2) #reduce row of one picture to one row (128, 3)\n",
        "  rgbSquaredSumOfBatch = rgbSquaredSumOfBatch.sum(axis=0) #combine all pictures of the batch to 3 values for rgb (3)\n",
        "\n",
        "  #squared sum of the batch for R, G, B is stored for final calculation\n",
        "  squaredSumOfBatchesR.append(rgbSquaredSumOfBatch[0])\n",
        "  squaredSumOfBatchesG.append(rgbSquaredSumOfBatch[1])\n",
        "  squaredSumOfBatchesB.append(rgbSquaredSumOfBatch[2])\n",
        "\n",
        "\n",
        "#final sum for R, G, B\n",
        "sumOfBatchesR = np.array(sumOfBatchesR).sum()\n",
        "sumOfBatchesG = np.array(sumOfBatchesG).sum()\n",
        "sumOfBatchesB = np.array(sumOfBatchesB).sum()\n",
        "\n",
        "#calculation of the mean for R, G, B\n",
        "meanR = sumOfBatchesR/(numberOfPixelsPerImage*noOfImages)\n",
        "meanG = sumOfBatchesG/(numberOfPixelsPerImage*noOfImages)\n",
        "meanB = sumOfBatchesB/(numberOfPixelsPerImage*noOfImages)\n",
        "\n",
        "#final squared sum for R, G, B\n",
        "squaredSumOfBatchesR = np.array(squaredSumOfBatchesR).sum()\n",
        "squaredSumOfBatchesG = np.array(squaredSumOfBatchesG).sum()\n",
        "squaredSumOfBatchesB = np.array(squaredSumOfBatchesB).sum()\n",
        "\n",
        "#final variance\n",
        "varianceR = (squaredSumOfBatchesR - ((sumOfBatchesR)**2)/(numberOfPixelsPerImage*noOfImages))/(numberOfPixelsPerImage*noOfImages)\n",
        "varianceG = (squaredSumOfBatchesG - ((sumOfBatchesG)**2)/(numberOfPixelsPerImage*noOfImages))/(numberOfPixelsPerImage*noOfImages)\n",
        "varianceB = (squaredSumOfBatchesB - ((sumOfBatchesB)**2)/(numberOfPixelsPerImage*noOfImages))/(numberOfPixelsPerImage*noOfImages)\n",
        "\n",
        "#final std of R, G, B\n",
        "stdR = math.sqrt(varianceR)\n",
        "stdG = math.sqrt(varianceG)\n",
        "stdB = math.sqrt(varianceB)\n",
        "\n",
        "print(meanR)\n",
        "print(meanG)\n",
        "print(meanB)\n",
        "print('---')\n",
        "print(stdR)\n",
        "print(stdG)\n",
        "print(stdB)\n",
        "  "
      ],
      "metadata": {
        "id": "Llcxfz0OHYov",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "6a05c071-124e-4bde-9b84-e44735f37e4b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-62d28e56f3bf>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#in calcBatch(es) with 128 pictures each perform sum operations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mcalcBatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaderCalc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m   \u001b[0;31m#calcBatch[0] with dimension (128, 3, 227, 227)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mcurrentBatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalcBatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-157e3163b2ff>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#image = Image.open(path).convert('RGB')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMREAD_UNCHANGED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#use this cell to avoid duration of the calculation of mean and std\n",
        "meanR = 0.5078696757850544\n",
        "meanG = 0.383553314333379\n",
        "meanB = 0.3425597905213773\n",
        "\n",
        "stdR = 0.2523046539381231\n",
        "stdG = 0.22919015141761262\n",
        "stdB =0.21377369693134016"
      ],
      "metadata": {
        "id": "BxtU3bsqavwC"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ssbwJ0mX1ZkX"
      },
      "outputs": [],
      "source": [
        "#TRANSFORMATION, TRAIN DATASET AND LOADER FOR MODEL TRAINING\n",
        "\n",
        "#transformation for model training and testing\n",
        "transformation = transforms.Compose([\n",
        "    #ToTensor() includes an automatic scaling from the interval [0,255] to [0.0,1.0]\n",
        "    transforms.ToTensor(),\n",
        "    #Normalization x_new = (x - mean)/std\n",
        "    transforms.Normalize(mean=[meanR, meanG, meanB], std=[stdR, stdG, stdB])\n",
        "])\n",
        "\n",
        "\n",
        "#datasets and dataloaders for model training and testing\n",
        "trainDataset = DDDdataset(train[:,0], train[:,-1], transformation)\n",
        "testDataset = DDDdataset(test[:,0], test[:,-1], transformation)\n",
        "\n",
        "trainDataLoader = DataLoader(trainDataset, batch_size=64, shuffle=True)\n",
        "testDataLoader = DataLoader(testDataset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#INFORMATION ABOUT CNN\n",
        "\n",
        "#transformation scales from interval [0, 255] to [0.0, 1.0] and then Normalization\n",
        "\n",
        "#batch_size = 64\n",
        "\n",
        "#----NUMBER OF PARAMETERS CNN below----\n",
        "\n",
        "# input image has size 227*227\n",
        "\n",
        "#conv1: 4*4*3*10 + 10 = 490 -> 4*4 filter, 3 input image channels, 10 number of filters, +10 bias parameters of each filter\n",
        "#conv2: 5*5*10*15 + 15 = 3765\n",
        "#conv3: 6*6*15*20 + 20 = 10820\n",
        "#conv4: 3*3*20*25 + 25 = 4525\n",
        "\n",
        "#after conv4 layer: output \"image\": 3*3*25 = 225\n",
        "\n",
        "#fc1: 225*45 + 45 = 10170 -> fc1 has 45 output features\n",
        "#fc2: 45*20 + 20 = 920\n",
        "#fc3: 20*2 + 2 = 42\n",
        "\n",
        "#TOTAL CNN: 30732 parameters\n",
        "\n",
        "#----NUMBER OF PARAMETERS CNN below----"
      ],
      "metadata": {
        "id": "F2rhmmMbrr7R"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "MY0t3t6Q1YV-"
      },
      "outputs": [],
      "source": [
        "#LAYERS OF THE MODEL\n",
        "\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=10, kernel_size=(4,4), stride=1, padding=1)\n",
        "    self.bn1 = nn.BatchNorm2d(10)\n",
        "    self.conv2 = nn.Conv2d(in_channels=10, out_channels=15, kernel_size=(5,5), stride=2, padding=1)\n",
        "    self.bn2 = nn.BatchNorm2d(15)\n",
        "    self.conv3 = nn.Conv2d(in_channels=15, out_channels=20, kernel_size=(6,6), stride=2, padding=1)\n",
        "    self.bn3 = nn.BatchNorm2d(20)\n",
        "    self.conv4 = nn.Conv2d(in_channels=20, out_channels=25, kernel_size=(3,3), stride=1, padding=1)\n",
        "    self.bn4 = nn.BatchNorm2d(25)\n",
        "\n",
        "    self.fc1 = nn.Linear(in_features=25*3*3, out_features=45)\n",
        "    self.bn5 = nn.BatchNorm1d(45)\n",
        "    self.fc2 = nn.Linear(in_features=45, out_features=20)\n",
        "    self.bn6 = nn.BatchNorm1d(20)\n",
        "    self.fc3 = nn.Linear(in_features=20, out_features=2)\n",
        "\n",
        "  def forward(self, X):\n",
        "\n",
        "    X = F.relu(self.bn1(self.conv1(X)))\n",
        "    X = F.max_pool2d(X, 2)\n",
        "    X = F.relu(self.bn2(self.conv2(X)))\n",
        "    X = F.max_pool2d(X, 2)\n",
        "    X = F.relu(self.bn3(self.conv3(X)))\n",
        "    X = F.max_pool2d(X, 3, stride=2)\n",
        "    X = F.relu(self.bn4(self.conv4(X)))\n",
        "    X = F.max_pool2d(X, 2)\n",
        "\n",
        "    X = X.view(X.shape[0], -1)\n",
        "    X = F.relu(self.bn5(self.fc1(X)))\n",
        "    X = F.relu(self.bn6(self.fc2(X)))\n",
        "    X = self.fc3(X)\n",
        "\n",
        "    return X\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "47OKopc4tK1K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d0c6051-7ab5-4dbf-acc0-d114e93db33e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " --- Epoch: 0, train loss: 0.0746, train acc: 0.9869, time: 130.72825121879578\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-96f23ff36f87>:81: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
            "  if rowsOfWrongClassifications == []:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, test loss: 0.1803, test acc: 0.9198, time: 174.6333134174347\n",
            "\n",
            "\n",
            " --- Epoch: 1, train loss: 0.0067, train acc: 0.9986, time: 275.72310042381287\n",
            "Epoch: 1, test loss: 0.0542, test acc: 0.9856, time: 314.69616198539734\n",
            "\n",
            "\n",
            " --- Epoch: 2, train loss: 0.0039, train acc: 0.9991, time: 415.0290915966034\n",
            "Epoch: 2, test loss: 0.0853, test acc: 0.9569, time: 453.78428840637207\n",
            "\n",
            "\n",
            " --- Epoch: 3, train loss: 0.0012, train acc: 0.9998, time: 554.7481293678284\n",
            "Epoch: 3, test loss: 0.0330, test acc: 0.9890, time: 592.6294622421265\n",
            "\n",
            "\n",
            " --- Epoch: 4, train loss: 0.0030, train acc: 0.9991, time: 693.494487285614\n",
            "Epoch: 4, test loss: 0.0815, test acc: 0.9749, time: 731.5033988952637\n",
            "\n",
            "\n",
            " --- Epoch: 5, train loss: 0.0025, train acc: 0.9993, time: 831.6628501415253\n",
            "Epoch: 5, test loss: 0.1177, test acc: 0.9429, time: 869.6619212627411\n",
            "\n",
            "\n",
            " --- Epoch: 6, train loss: 0.0004, train acc: 1.0000, time: 968.090963602066\n",
            "Epoch: 6, test loss: 0.0303, test acc: 0.9883, time: 1005.829808473587\n",
            "\n",
            "\n",
            " --- Epoch: 7, train loss: 0.0014, train acc: 0.9996, time: 1104.124524116516\n",
            "Epoch: 7, test loss: 0.1341, test acc: 0.9402, time: 1141.9995331764221\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#MODEL TRAINING AND TESTING\n",
        "\n",
        "import time\n",
        "\n",
        "#basic settings\n",
        "model = CNN().to(device)\n",
        "number_epochs = 8\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "losses = []\n",
        "accuracies = []\n",
        "\n",
        "\n",
        "#for the analysis of the wrong classifications\n",
        "pathsOfWrongClassifications = []\n",
        "rowsOfWrongClassifications = []\n",
        "correctLabel = []\n",
        "epochOfWrongClassification = []\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "#model training and testing process\n",
        "for epoch in range(number_epochs):\n",
        "\n",
        "\n",
        "  epoch_loss = 0\n",
        "  epoch_accuracy = 0\n",
        "  \n",
        "  #-------model training-------\n",
        "  for (X, y, path) in trainDataLoader:\n",
        "    X = X.to(device) #torch.as_tensor(X)\n",
        "    y = y.to(device) #torch.as_tensor(y)\n",
        "    predictions = model(X)\n",
        "    loss = loss_func(predictions, y.long())\n",
        "\n",
        "    #adaption of the neural network\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    #\n",
        "    accuracy = ((predictions.argmax(dim=1) == y).float().mean())\n",
        "    epoch_accuracy += accuracy\n",
        "    epoch_loss += loss\n",
        "  \n",
        "  #total epoch_accuracy and epoch_loss calculation\n",
        "  epoch_accuracy = epoch_accuracy/len(trainDataLoader)\n",
        "  accuracies.append(epoch_accuracy)\n",
        "  epoch_loss = epoch_loss / len(trainDataLoader)\n",
        "  losses.append(epoch_loss)\n",
        "\n",
        "  print(\"\\n --- Epoch: {}, train loss: {:.4f}, train acc: {:.4f}, time: {}\".format(epoch, epoch_loss, epoch_accuracy, time.time() - start))\n",
        "  #-------model training-------\n",
        "  \n",
        "  \n",
        "  #---model testing---\n",
        "  model.eval() #sets the model in evaluation mode\n",
        "  \n",
        "  #with disabled gradient calculation\n",
        "  with torch.no_grad():\n",
        "    test_epoch_loss = 0\n",
        "    test_epoch_accuracy = 0\n",
        "\n",
        "    for test_X, test_y, path in testDataLoader:\n",
        "\n",
        "      test_X = test_X.to(device)\n",
        "      test_y = test_y.to(device)\n",
        "\n",
        "      test_preds = model(test_X)\n",
        "      \n",
        "      test_loss = loss_func(test_preds, test_y.long())\n",
        "\n",
        "      test_epoch_loss += test_loss            \n",
        "      test_accuracy = ((test_preds.argmax(dim=1) == test_y).float().mean())\n",
        "      test_epoch_accuracy += test_accuracy\n",
        "\n",
        "      #if test prediction is wrong save the path of the image that has been classified wrong\n",
        "      for index, row in enumerate(test_preds):\n",
        "        if row.argmax(dim=0) != test_y[index]:\n",
        "          pathsOfWrongClassifications.append(path[index])\n",
        "          if rowsOfWrongClassifications == []:\n",
        "            row = row.cpu()\n",
        "            rowsOfWrongClassifications = row\n",
        "          else:\n",
        "            row = row.cpu()\n",
        "            rowsOfWrongClassifications = np.vstack([rowsOfWrongClassifications, row])\n",
        "          correctLabel.append(test_y[index].item())\n",
        "          epochOfWrongClassification.append(epoch)\n",
        "\n",
        "    #total test_epoch_accuracy and test_epoch_loss calculation\n",
        "    test_epoch_accuracy = test_epoch_accuracy/len(testDataLoader)\n",
        "    test_epoch_loss = test_epoch_loss / len(testDataLoader)\n",
        "\n",
        "    print(\"Epoch: {}, test loss: {:.4f}, test acc: {:.4f}, time: {}\\n\".format(epoch, test_epoch_loss, test_epoch_accuracy, time.time() - start))\n",
        "  #---model testing---\n",
        "\n",
        "  model.train(mode=True) #sets the model back in training mode\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DATAFRAME OF ALL PERSONS THAT HAVE BEEN CLASSIFIED WRONG IN ALL EPOCHS\n",
        "\n",
        "#extract the person ID out of the stored image paths\n",
        "personID = []\n",
        "for pathStr in pathsOfWrongClassifications:\n",
        "  personID.append(pathStr[pathStr.rfind('/')+1:-8])\n",
        "\n",
        "\n",
        "#create a dataframe from all wrong classified images in the test dataset\n",
        "dfWrong = pd.DataFrame()\n",
        "dfWrong['testEpochNo'] = epochOfWrongClassification\n",
        "dfWrong['personID'] = personID\n",
        "dfWrong['imagepath'] = pathsOfWrongClassifications\n",
        "dfWrong['correctLabel'] = correctLabel\n",
        "dfWrong['outputValueLabel0'] = rowsOfWrongClassifications[:,0]\n",
        "dfWrong['outputValueLabel1'] = rowsOfWrongClassifications[:,1]\n",
        "\n",
        "print(dfWrong)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YWg6fM1IoVT",
        "outputId": "eca0d5a0-2a95-45f8-fd67-11d955e4320e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      testEpochNo personID                                          imagepath  \\\n",
            "0               0        E   Driver Drowsiness Dataset (DDD)/Drowsy/E0467.png   \n",
            "1               0        E   Driver Drowsiness Dataset (DDD)/Drowsy/E0077.png   \n",
            "2               0        E   Driver Drowsiness Dataset (DDD)/Drowsy/E0531.png   \n",
            "3               0        E   Driver Drowsiness Dataset (DDD)/Drowsy/E0508.png   \n",
            "4               0        E   Driver Drowsiness Dataset (DDD)/Drowsy/E0103.png   \n",
            "...           ...      ...                                                ...   \n",
            "3358            7        e  Driver Drowsiness Dataset (DDD)/Non Drowsy/e06...   \n",
            "3359            7        E   Driver Drowsiness Dataset (DDD)/Drowsy/E0926.png   \n",
            "3360            7        E   Driver Drowsiness Dataset (DDD)/Drowsy/E0493.png   \n",
            "3361            7        E   Driver Drowsiness Dataset (DDD)/Drowsy/E0890.png   \n",
            "3362            7        e  Driver Drowsiness Dataset (DDD)/Non Drowsy/e06...   \n",
            "\n",
            "      correctLabel  outputValueLabel0  outputValueLabel1  \n",
            "0                0          -0.395040           0.171429  \n",
            "1                0          -0.425977           0.138802  \n",
            "2                0          -0.987536           0.470461  \n",
            "3                0          -0.445469           0.152856  \n",
            "4                0          -0.294095           0.050403  \n",
            "...            ...                ...                ...  \n",
            "3358             1           0.235244          -0.096408  \n",
            "3359             0          -2.409271           1.460404  \n",
            "3360             0          -2.763428           1.669146  \n",
            "3361             0          -2.336305           1.417878  \n",
            "3362             1           0.091763           0.020013  \n",
            "\n",
            "[3363 rows x 6 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get an overview of the persons that are likely to be classified in a wrong way (from last epoch)\n",
        "dfOfEpoch = dfWrong[dfWrong['testEpochNo']==7]\n",
        "dfOfEpoch.groupby('personID').size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Pg3VzoDQBqb",
        "outputId": "888639ad-b049-4b0e-d3b3-06bc150f60dd"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "personID\n",
              "E    338\n",
              "S      2\n",
              "e    316\n",
              "g      7\n",
              "u      1\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results of older different trials:\n",
        "\n",
        "\n",
        "I)\n",
        "\n",
        "2 epochs\n",
        "\n",
        "persons that have been shifted totally to the training dataset:\n",
        "e, E, g, G\n",
        "\n",
        "wrong classifications:\n",
        "\n",
        "E: 5/962 = 0.5%\n",
        "\n",
        "G: 48/499 = 9.6%\n",
        "\n",
        "e: 1000/1000 = 100%\n",
        "\n",
        "g: 97/109 = 89.0%\n",
        "\n",
        "s: 18/457 = 3.9%\n",
        "\n",
        "S: 0/487 = 0.0%\n",
        "\n",
        "all wrong classifications:\n",
        "\n",
        "personID\n",
        "\n",
        "E       5\n",
        "\n",
        "G      48\n",
        "\n",
        "b       7\n",
        "\n",
        "c       1\n",
        "\n",
        "e    1000\n",
        "\n",
        "g      97\n",
        "\n",
        "n      10\n",
        "\n",
        "o       2\n",
        "\n",
        "r       2\n",
        "\n",
        "s      18\n",
        "\n",
        "u       8\n",
        "\n",
        "w       5\n",
        "\n",
        "x       1\n",
        "\n",
        "y       3\n",
        "\n",
        "II)\n",
        "\n",
        "3 epochs\n",
        "\n",
        "persons that have been shifted totally to the training dataset:\n",
        "e, E, g, G, s, S\n",
        "\n",
        "wrong classifications:\n",
        "\n",
        "E: 888/962 = 92.3%\n",
        "\n",
        "G: 0/499 = 0.0%\n",
        "\n",
        "e: 854/1000 = 85.4%\n",
        "\n",
        "g: 11/109 = 10.1%\n",
        "\n",
        "s: 402/457 = 88.0%\n",
        "\n",
        "S: 419/487 = 86.0%\n",
        "\n",
        "\n",
        "all wrong classifications\n",
        "\n",
        "personID\n",
        "\n",
        "E    888\n",
        "\n",
        "S    419\n",
        "\n",
        "e    854\n",
        "\n",
        "g     11\n",
        "\n",
        "s    402\n",
        "\n"
      ],
      "metadata": {
        "id": "HTPa784Zi5rj"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}